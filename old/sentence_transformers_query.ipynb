{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_olympics = pd.read_csv('docs/olympics_sections_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_olympics['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3561416e78c42ff99e0408ee6ff2ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_emb_list = []\n",
    "for d in tqdm(docs):\n",
    "    doc_emb_list.append(encode(d).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emb = torch.tensor(doc_emb_list).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3964, 384])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(doc_emb, 'olympics_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context separator contains 3 tokens'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "MAX_SECTION_LEN = 500\n",
    "SEPARATOR = \"\\n* \"\n",
    "ENCODING = \"cl100k_base\"  # encoding for text-embedding-ada-002\n",
    "\n",
    "encoding = tiktoken.get_encoding(ENCODING)\n",
    "separator_len = len(encoding.encode(SEPARATOR))\n",
    "\n",
    "f\"Context separator contains {separator_len} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "query = \"Who won the women's long jump?\"\n",
    "#Encode query and docs\n",
    "query_emb = encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66 The women's long jump event at the 2020 Summer Olympics took place on 1 and 3 August 2021 at the Japan National Stadium. 30 athletes from 23 nations competed. Germany's 2019 world champion Malaika Mihambo moved up from third  to first with her final round jump of 7.00 metres, to win the gold medal. 2012 Olympic champion Brittney Reese of the USA won the silver and Nigeria's Ese Brume the bronze.\n",
      "0.615 The women's triple jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Japan National Stadium.The event was won by Yulimar Rojas of Venezuela: Her winning jump of 15.67 meters also broke the 26-year-old world record.\n",
      "0.587 The men's long jump event at the 2020 Summer Olympics took place between 31 July and 2 August 2021 at the Japan National Stadium. Approximately 35 athletes were expected to compete; the exact number was dependent on how many nations use universality places to enter athletes in addition to the 32 qualifying through time or ranking (1 universality place was used in 2016). 31 athletes from 20 nations competed. Miltiadis Tentoglou won the gold medal, Greece's first medal in the men's long jump. Cuban athletes Juan Miguel Echevarría and Maykel Massó earned silver and bronze, respectively, the nation's first medals in the event since 2008.\n",
      "0.57 The women's high jump event at the 2020 Summer Olympics took place on 5 and 7 August 2021 at the Japan National Stadium. Even though 32 athletes qualified through the qualification system for the Games, only 31 took part in the competition. This was the 22nd appearance of the event, having appeared at every Olympics since women's athletics was introduced in 1928.\n",
      "0.56 The U.S. Olympic jumping team was named on July 5, 2021. The team consisted of two Olympic veterans, Kent Farrington and Laura Kraut, who were joined by rookie Jessica Springsteen.\n"
     ]
    }
   ],
   "source": [
    "contexts = []\n",
    "top_n = 5\n",
    "\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs[:top_n]:\n",
    "    print(round(score,3), doc)\n",
    "    contexts.append(doc)\n",
    "\n",
    "joint_context = SEPARATOR.join(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "prompt =  header + joint_context + \"\\n\\n Q: \" + query + \"\\n A:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import toml\n",
    "\n",
    "secrets = toml.load('.streamlit/secrets.toml')\n",
    "\n",
    "openai_api_key = secrets['openai_api_key']\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "\n",
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Malaika Mihambo won the women's long jump.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "response[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6g3luFyMUQzvZcpR5gKoWxoMIZoJM at 0x7f1b7c722680> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" Malaika Mihambo won the women's long jump.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1675482294,\n",
       "  \"id\": \"cmpl-6g3luFyMUQzvZcpR5gKoWxoMIZoJM\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 13,\n",
       "    \"prompt_tokens\": 449,\n",
       "    \"total_tokens\": 462\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The men\\'s high jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Olympic Stadium. 33 athletes from 24 nations competed; the total possible number depended on how many nations would use universality places to enter athletes in addition to the 32 qualifying through mark or ranking (no universality places were used in 2021). Italian athlete Gianmarco Tamberi along with Qatari athlete Mutaz Essa Barshim emerged as joint winners of the event following a tie between both of them as they cleared 2.37m. Both Tamberi and Barshim agreed to share the gold medal in a rare instance where the athletes of different nations had agreed to share the same medal in the history of Olympics. Barshim in particular was heard to ask a competition official \"Can we have two golds?\" in response to being offered a \\'jump off\\'. Maksim Nedasekau of Belarus took bronze. The medals were the first ever in the men\\'s high jump for Italy and Belarus, the first gold in the men\\'s high jump for Italy and Qatar, and the third consecutive medal in the men\\'s high jump for Qatar (all by Barshim). Barshim became only the second man to earn three medals in high jump, joining Patrik Sjöberg of Sweden (1984 to 1992).\\n* The women\\'s triple jump event at the 2020 Summer Olympics took place between 30 July and 1 August 2021 at the Japan National Stadium.The event was won by Yulimar Rojas of Venezuela: Her winning jump of 15.67 meters also broke the 26-year-old world record.\\n* The women\\'s high jump event at the 2020 Summer Olympics took place on 5 and 7 August 2021 at the Japan National Stadium. Even though 32 athletes qualified through the qualification system for the Games, only 31 took part in the competition. This was the 22nd appearance of the event, having appeared at every Olympics since women\\'s athletics was introduced in 1928.\\n* This was the 29th appearance of the event, which is one of 12 athletics events to have been held at every Summer Olympics.\\nNo nations made their men\\'s triple jump debut. The United States competed for the 28th time, having missed only the boycotted 1980 Games.\\n* The U.S. Olympic jumping team was named on July 5, 2021. The team consisted of two Olympic veterans, Kent Farrington and Laura Kraut, who were joined by rookie Jessica Springsteen.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use openai ada embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL):\n",
    "    result = openai.Embedding.create(\n",
    "      model=model,\n",
    "      input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Who won the women's long jump?\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ada_emb = get_embedding(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "query_ada_emb_np = np.array(query_ada_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_ada_emb_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('docs/govtext/md/summarisation.md', 'r') as f:\n",
    "with open('docs/govtext/raw_text/summarisation.txt', 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = soup.get_text()\n",
    "cleaned_text = re.sub('\\n+', '\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nsidebar_position: 7\\n---\\n# Summarisation\\n## CONCEPT\\nGovText offers two summarisation options:\\n1. **Normal**: using abstractive summarisation, the main points of a document are consolidated and paraphrased into a short paragraph which reads like a human written one  \\n2. **Quick**: using extractive summarisation, the most important sentences of a document are \"lifted\" and highlighted\\nDue to the complexity involved in performing an abstractive summarisation (normal summary), it takes up more computing resources and time. Therefore, users need to enter the maximum length (number of words) of the summary they want, and only one summary which is shorter or equal to this length will be returned.  \\nExtractive summarisation (quick summary) is very much faster than abstractive summarisation. When this option is activated, summaries of the following lengths will be returned for each document: \\n1. Short (around 15% of original document length) \\n2. Medium (around 30% of original document length)\\n3. Long (around 45% of original document length)\\nThe abstractive summarisation uses models pre-trained on summaries of news articles. Therefore, abstractive summarisation works best on similar articles. \\n## SUBMIT AN ANALYSIS\\nClick on the **Predictions** button in the navigation bar, then select the **Summarisation** card.\\nOn the Summarisation submission page:\\n1. Enter a name for your analysis\\n2. Select a dataset from the existing dataset list \\n3. Change the configuration based on your needs\\n4. Click **Submit**\\nIf the submission is successful, a message will appear to inform you of this.\\nThe animation below demonstrates this process. \\n![Summarisation_Config](/img/Summarisation1_Config.gif)\\nThe maximum number of predictions in the **Created** or **In Progress** statuses is capped at 3 per user. That means that until 1 of the predictions is completed, you will not be allowed to submit any more predictions. \\nWhen submitting a prediction, you will be informed of the number of predictions that are still being processed.\\n![CountInProgress](/img/TopicModelling_Submit2.JPG)\\n### Select Dataset\\nOn the Summarisation submission page, you can find the full list of datasets that you have already uploaded. Select the dataset that you will be using for this analysis. \\nYou can do a search using the dataset name.\\n![Summarisation_DataFilter](/img/Summarisation2_DataFilter.gif)\\nDo note that **only the first 50 documents in the Excel or CSV file will be processed**. Any documents after the 51st row in the spreadsheet will be discarded for summarisation. \\nIf there are multiple documents in the Excel or CSV file (1 document per row in the file), each document will be summarised individually. \\nIf you need to upload a new dataset, click on the **Upload New Dataset** button or the **Dataset** button in the navigation bar, to reach the  Dataset Upload page. The steps to upload a dataset can be found in the [Datasets](datasets) section. \\nAfter successfully uploading your new dataset, you will need to return to the Summarisation submission page.\\n### Summarisation Options\\nAs mentioned above, GovText offers two types of summarisation: \\n1. By default (Quick Summary not checked), abstractive summarisation will be performed. This will return paraphrased summaries of the documents\\' main points but the time taken is longer. For this option, you will need to select the maximum summary length.\\n    Do note that the summaries returned might not be close to the selected maximum summary length (but they will definitely not be more than this length). This is because the model score the words in the summary and decide on the optimum length which provides the most coherent one. From our experiments, the model produces the most coherent summaries with lengths around 200 words.\\n2. If you select the Quick Summary option, key sentences from the documents will be highlighted using extractive summarisation. There is no need to select the maximum summary length for this option.\\n![Summarisation_Config](/img/Summarisation4_Config.JPG)\\n## CHECKING PROCESSING STATUS\\nClick on the **Predictions** button on the navigation bar. \\nUnder the List of Predictions table, you will see the processing status of all your submitted predictions. \\nTo view the results of a completed prediction, click on the corresponding row. \\n## VIEWING THE SUMMARY RESULTS\\n### Normal (Abstractive) Summary\\nFor normal summaries, the main points are paraphrased and the summaries and presented as short paragraphs in the **Summary** panel.\\nIf there are multiple documents in your dataset, use the arrows at the bottom of the page to switch between documents.\\n![Summarisation_Result_Abstractive](/img/Summarisation5_Results_Abs.JPG)\\n### Quick (Extractive) Summary\\nIf the Quick Summary option is selected, the summaries will be in point forms, and the corresponding sentences in the original document will be highlighted.\\nThree versions of the summaries will be shown - short, medium, and long. Click on the summary length buttons above the summary panel to switch between the versions.  \\n![Summarisation_Result_Extractive](/img/Summarisation6_Results_Ext.JPG)\\n### Other Utilities\\nThe **Download all results** icon is for downloading the full set of results.\\nThe **Create new prediction** button is a shortcut to create a new prediction with the exact same settings as the current prediction.\\n![Summarisation_Result_buttons](/img/Summarisation7_2buttons.gif)\\n## DOWNLOAD ALL RESULTS\\nClick the **Download all results** button above the Original Document panel. \\nThe summaries of all documents in the analysed dataset will be downloaded as a zip file. Please uncompress the file to view them.\\nThe unzipped Excel file for the Normal (Abstractive) Summary results has 3 columns:\\n* Column A: Document ID\\n* Column B: Original Document\\n* Column C: Summary\\n![Summarisation_Result_DownloadAbstractive](/img/Summarisation8_DownloadAbs.JPG)\\nThe unzipped Excel file for the Quick (Extractive) Summary results has 5 columns:\\n* Column A: Document ID\\n* Column B: Original Document\\n* Column C: Short Summary\\n* Column D: Medium Summary\\n* Column E: Long Summary\\n![Summarisation_Result_DownloadExtractive](/img/Summarisation9_DownloadExt.JPG)'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model scores the words in the summary and decides on the optimum length which provides the most coherent one. From our experiments, the model produces the most coherent summaries with lengths around 200 words.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_query = \"Why are the results not close to my maximum length?\"\n",
    "summarization_prompt =  header + cleaned_text + \"\\n\\n Q: \" + summarization_query + \"\\n A:\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "                prompt=summarization_prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "response[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_sections_len = 0\n",
    "    chosen_sections_indexes = []\n",
    "     \n",
    "    for _, section_index in most_relevant_document_sections:\n",
    "        # Add contexts until we run out of space.        \n",
    "        document_section = df.loc[section_index]\n",
    "        \n",
    "        chosen_sections_len += document_section.tokens + separator_len\n",
    "        if chosen_sections_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(SEPARATOR + document_section.content.replace(\"\\n\", \" \"))\n",
    "        chosen_sections_indexes.append(str(section_index))\n",
    "            \n",
    "    # Useful diagnostic information\n",
    "    print(f\"Selected {len(chosen_sections)} document sections:\")\n",
    "    print(\"\\n\".join(chosen_sections_indexes))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
